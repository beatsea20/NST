{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgLmd78THVj7"
   },
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "  \n",
    "  @classmethod\n",
    "  def custom_model(cls):\n",
    "    filters = [64, 128, 64, 32]\n",
    "    layer_names = [f\"conv_block_{i}\" for i in range(2, 2 + len(filters))]\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape = (256, 256, 3),\n",
    "                                        name = \"input_layer\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters = 32,\n",
    "                               kernel_size = (2, 2),\n",
    "                               strides = (1, 1),\n",
    "                               padding = \"same\",\n",
    "                               name = \"conv_block_1\")(input_layer)\n",
    "                   \n",
    "    layer_num = len(layer_names) or len(filters)\n",
    "    activation_layers = list()\n",
    "    for i in range(layer_num):\n",
    "      if i % 2 == 1 and i != 0:\n",
    "        x = tf.keras.layers.ReLU(name = f\"relu_layer_{i}th_iter\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "      x = tf.keras.layers.Conv2D(filters = filters[i],\n",
    "                               kernel_size = (3, 3),\n",
    "                               strides = (1, 1),\n",
    "                               padding = \"same\",\n",
    "                               name = layer_names[i])(x)\n",
    "      \n",
    "      activation_layers.append(x)\n",
    "                                  \n",
    "    out = tf.keras.layers.Conv2D(filters = 3,\n",
    "                               kernel_size = (3, 3),\n",
    "                               strides = (2, 2),\n",
    "                               padding = \"same\",\n",
    "                               name = \"conv_block_out\")(x)\n",
    "\n",
    "    activation_layers.append(out)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = activation_layers,\n",
    "                                  name = \"feature_extractor_model\")\n",
    "\n",
    "    return model         \n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def vgg_extractor_model(cls):\n",
    "    vgg_model = tf.keras.applications.VGG19(include_top = False,\n",
    "                                            weights = \"imagenet\")\n",
    "    \n",
    "    style_conv_blocks = [f\"block{i}_conv1\" for i in range(1, 6)]\n",
    "    content_conv_block = [\"block5_conv2\"]\n",
    "    all_activation_layers = style_conv_blocks + content_conv_block\n",
    "\n",
    "    input_layer = vgg_model.inputs\n",
    "    output_layers = [vgg_model.get_layer(i).output for i in all_activation_layers]\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = output_layers,\n",
    "                                  name = \"vgg19_extractor\")\n",
    "\n",
    "    return model\n",
    "\n",
    "  \n",
    "  @classmethod\n",
    "  def vgg_with_recurrent(cls):\n",
    "    input_layer = tf.keras.layers.Input(shape = (256, 256, 3))\n",
    "    vgg_layer = tf.keras.applications.VGG19(include_top = False,\n",
    "                                            weights = \"imagenet\")(input_layer)\n",
    "\n",
    "    content_activation = tf.keras.layers.ConvLSTM2D(64, (1, 1), (1, 1),\n",
    "                                                    padding = \"same\")    \n",
    "    \n",
    "    num_layers = 1\n",
    "    recurrent_layer_names = [f\"rnn_conv_{i}\" for i in range(1, num_layers)]\n",
    "    recurrent_layers = list()\n",
    "    for layer_name in recurrent_layer_names:\n",
    "      rnn_layer = tf.keras.layers.ConvLSTM2D(64, (1, 1), (1, 1), padding = \"same\",\n",
    "                                             name = layer_name)\n",
    "      recurrent_layers.append(rnn_layer)\n",
    "\n",
    "    x = input_layer\n",
    "    model_outputs = list()\n",
    "    for rnn_layer in recurrent_layers:\n",
    "      x = tf.expand_dims(x, axis = 0)\n",
    "      x = rnn_layer(x)\n",
    "      model_outputs.append(x)\n",
    "\n",
    "    x = tf.expand_dims(x, axis = 0)\n",
    "    out = content_activation(x)\n",
    "    model_outputs.append(out)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = model_outputs,\n",
    "                                  name = \"vgg_19_with_recurrent\")\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "  @classmethod\n",
    "  def extract(cls, image_stack, model):\n",
    "    \"\"\"Image stack is (3, None, None, 3)\n",
    "    shaped image data which contains\n",
    "    content, style and generated images\"\"\"\n",
    "    return model()(image_stack)\n",
    "\n",
    "  \n",
    "  @staticmethod\n",
    "  def get_layers(features):\n",
    "    content = features[-1]\n",
    "    style = features[:-1]\n",
    "    \n",
    "    return content, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "  \n",
    "  @classmethod\n",
    "  def gram_matrix(cls, arr):\n",
    "    \"\"\"Gramian matrix for calculating style loss\"\"\"\n",
    "    x = tf.transpose(arr, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def content_loss(cls, content, generated):\n",
    "    \"\"\"1/2 * sum of (generated - original) ** 2\"\"\"\n",
    "    content_loss = tf.reduce_sum(tf.square((generated - content)))\n",
    "    \n",
    "    return content_loss * 5e-1\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def style_loss(cls, style, generated):\n",
    "    style_gram = cls.gram_matrix(style)\n",
    "    generated_gram = cls.gram_matrix(generated)\n",
    "\n",
    "    style_loss = tf.reduce_mean(tf.square(generated_gram - style_gram))\n",
    "\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from loss import Loss\n",
    "from feature_extractor import FeatureExtractor\n",
    "\n",
    "\n",
    "class Constants(object):\n",
    "  CONTENT_WEIGHT = 2e-5\n",
    "  STYLE_WEIGHT = 1e-4\n",
    "\n",
    "\n",
    "class Train(Loss, FeatureExtractor):\n",
    "  \n",
    "  @classmethod\n",
    "  def calculate_step_loss(cls, model, content, style, generated):\n",
    "    tensor = tf.concat([content, style, generated], axis = 0)\n",
    "    features = cls.extract(image_stack = tensor, model = model)\n",
    "    content_act, style_act = cls.get_layers(features)\n",
    "\n",
    "    content_loss = cls.content_loss(content_act[0], content_act[-1])\n",
    "\n",
    "    style_loss = 0.\n",
    "    for layer in style_act:\n",
    "      layer_loss = cls.style_loss(layer[1], layer[-1])\n",
    "      style_loss += layer_loss\n",
    "\n",
    "    loss = (content_loss * Constants.CONTENT_WEIGHT) \\\n",
    "           + (style_loss * Constants.STYLE_WEIGHT)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, content, style, generated, epochs = 10):\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-4) \n",
    "  for epoch in range(epochs):\n",
    "    with tf.GradientTape() as GT:\n",
    "      loss = Train.calculate_step_loss(model, content, style, generated)\n",
    "\n",
    "    print(f\"EPOCH: {epoch + 1} \\nLOSS: {loss}\\n\" + (\"---\" * 15))\n",
    "\n",
    "    gradients = GT.gradient(loss, generated)\n",
    "    optimizer.apply_gradients([(gradients, generated)])\n",
    "\n",
    "  return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_image(path: str):\n",
    "  image = io.imread(path)\n",
    "  expanded = tf.expand_dims(tf.cast(tf.convert_to_tensor(image),\n",
    "                                    tf.float32) / 255., axis = 0)\n",
    "\n",
    "  return expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quoAfx27Ts5a"
   },
   "source": [
    "### All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3860,
     "status": "ok",
     "timestamp": 1621584804639,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "fOB2X21yICcv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "\n",
    "# rewrite the paths for your own images and base folder\n",
    "BASE = \"./drive/MyDrive/neural-style-transfer\"\n",
    "CONTENT_IMAGE_PATH = os.path.join(BASE, \"images\", \"content.jpeg\")\n",
    "STYLE_IMAGE_PATH = os.path.join(BASE, \"images\", \"style.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aMgBhEVTqGU"
   },
   "source": [
    "### Loading Images - I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5195,
     "status": "ok",
     "timestamp": 1621584805992,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "cCrf7VCW08Tj"
   },
   "outputs": [],
   "source": [
    "def load_image(path: str):\n",
    "  image = io.imread(path)\n",
    "  expanded = tf.expand_dims(tf.cast(tf.convert_to_tensor(image),\n",
    "                                    tf.float32) / 255., axis = 0)\n",
    "\n",
    "  return expanded\n",
    "\n",
    "class Constants(object):\n",
    "  CONTENT_WEIGHT = 2e-5\n",
    "  STYLE_WEIGHT = 1e-4\n",
    "\n",
    "CONTENT = load_image(CONTENT_IMAGE_PATH)\n",
    "STYLE = load_image(STYLE_IMAGE_PATH)\n",
    "COMBINED = tf.Variable(CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zyuMcY5Tmwj"
   },
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5189,
     "status": "ok",
     "timestamp": 1621584805994,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "7yxE-zHnFc0z"
   },
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "\n",
    "  @classmethod\n",
    "  def gram_matrix(cls, arr):\n",
    "    \"\"\"Gramian matrix for calculating style loss\"\"\"\n",
    "    x = tf.transpose(arr, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    \n",
    "    return gram\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def content_loss(cls, content, generated):\n",
    "    \"\"\"1/2 * sum of (generated - original) ** 2\"\"\"\n",
    "    content_loss = tf.reduce_sum(tf.square((generated - content)))\n",
    "    \n",
    "    return content_loss * 5e-1\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def style_loss(cls, style, generated):\n",
    "    style_gram = cls.gram_matrix(style)\n",
    "    generated_gram = cls.gram_matrix(generated)\n",
    "\n",
    "    style_loss = tf.reduce_mean(tf.square(generated_gram - style_gram))\n",
    "\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCepDGlhTlAu"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5185,
     "status": "ok",
     "timestamp": 1621584805997,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "yKzn8KIvKnqi"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "  \n",
    "  @classmethod\n",
    "  def custom_model(cls):\n",
    "    filters = [64, 128, 64, 32]\n",
    "    layer_names = [f\"conv_block_{i}\" for i in range(2, 2 + len(filters))]\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape = (256, 256, 3),\n",
    "                                        name = \"input_layer\")\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters = 32,\n",
    "                               kernel_size = (2, 2),\n",
    "                               strides = (1, 1),\n",
    "                               padding = \"same\",\n",
    "                               name = \"conv_block_1\")(input_layer)\n",
    "                   \n",
    "    layer_num = len(layer_names) or len(filters)\n",
    "    activation_layers = list()\n",
    "    for i in range(layer_num):\n",
    "      if i % 2 == 1 and i != 0:\n",
    "        x = tf.keras.layers.ReLU(name = f\"relu_layer_{i}th_iter\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "      x = tf.keras.layers.Conv2D(filters = filters[i],\n",
    "                               kernel_size = (3, 3),\n",
    "                               strides = (1, 1),\n",
    "                               padding = \"same\",\n",
    "                               name = layer_names[i])(x)\n",
    "      \n",
    "      activation_layers.append(x)\n",
    "                                  \n",
    "    out = tf.keras.layers.Conv2D(filters = 3,\n",
    "                               kernel_size = (3, 3),\n",
    "                               strides = (2, 2),\n",
    "                               padding = \"same\",\n",
    "                               name = \"conv_block_out\")(x)\n",
    "\n",
    "    activation_layers.append(out)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = activation_layers,\n",
    "                                  name = \"feature_extractor_model\")\n",
    "\n",
    "    return model         \n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def vgg_extractor_model(cls):\n",
    "    vgg_model = tf.keras.applications.VGG19(include_top = False,\n",
    "                                            weights = \"imagenet\")\n",
    "    \n",
    "    style_conv_blocks = [f\"block{i}_conv1\" for i in range(1, 6)]\n",
    "    content_conv_block = [\"block5_conv2\"]\n",
    "    all_activation_layers = style_conv_blocks + content_conv_block\n",
    "\n",
    "    input_layer = vgg_model.inputs\n",
    "    output_layers = [vgg_model.get_layer(i).output for i in all_activation_layers]\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = output_layers,\n",
    "                                  name = \"vgg19_extractor\")\n",
    "\n",
    "    return model\n",
    "\n",
    "  \n",
    "  @classmethod\n",
    "  def vgg_with_recurrent(cls):\n",
    "    input_layer = tf.keras.layers.Input(shape = (256, 256, 3))\n",
    "    vgg_layer = tf.keras.applications.VGG19(include_top = False,\n",
    "                                            weights = \"imagenet\")(input_layer)\n",
    "\n",
    "    content_activation = tf.keras.layers.ConvLSTM2D(64, (1, 1), (1, 1),\n",
    "                                                    padding = \"same\")    \n",
    "    \n",
    "    num_layers = 1\n",
    "    recurrent_layer_names = [f\"rnn_conv_{i}\" for i in range(1, num_layers)]\n",
    "    recurrent_layers = list()\n",
    "    for layer_name in recurrent_layer_names:\n",
    "      rnn_layer = tf.keras.layers.ConvLSTM2D(64, (1, 1), (1, 1), padding = \"same\",\n",
    "                                             name = layer_name)\n",
    "      recurrent_layers.append(rnn_layer)\n",
    "\n",
    "    x = input_layer\n",
    "    model_outputs = list()\n",
    "    for rnn_layer in recurrent_layers:\n",
    "      x = tf.expand_dims(x, axis = 0)\n",
    "      x = rnn_layer(x)\n",
    "      model_outputs.append(x)\n",
    "\n",
    "    x = tf.expand_dims(x, axis = 0)\n",
    "    out = content_activation(x)\n",
    "    model_outputs.append(out)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = input_layer,\n",
    "                                  outputs = model_outputs,\n",
    "                                  name = \"vgg_19_with_recurrent\")\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "  @classmethod\n",
    "  def extract(cls, image_stack, model):\n",
    "    \"\"\"Image stack is (3, None, None, 3)\n",
    "    shaped image data which contains\n",
    "    content, style and generated images\"\"\"\n",
    "    return model()(image_stack)\n",
    "\n",
    "  \n",
    "  @staticmethod\n",
    "  def get_layers(features):\n",
    "    content = features[-1]\n",
    "    style = features[:-1]\n",
    "    \n",
    "    return content, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufJKeL2ETgrm"
   },
   "source": [
    "### Training step - calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5180,
     "status": "ok",
     "timestamp": 1621584805999,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "nMK9jY_WysYn"
   },
   "outputs": [],
   "source": [
    "class Train(Loss, FeatureExtractor):\n",
    "\n",
    "  @classmethod\n",
    "  def calculate_step_loss(cls, model, content, style, generated):\n",
    "    tensor = tf.concat([content, style, generated], axis = 0)\n",
    "    features = cls.extract(image_stack = tensor, model = model)\n",
    "    content_act, style_act = cls.get_layers(features)\n",
    "\n",
    "    content_loss = cls.content_loss(content_act[0], content_act[-1])\n",
    "\n",
    "    style_loss = 0.\n",
    "    for layer in style_act:\n",
    "      layer_loss = cls.style_loss(layer[1], layer[-1])\n",
    "      style_loss += layer_loss\n",
    "\n",
    "    loss = (content_loss * Constants.CONTENT_WEIGHT) \\\n",
    "           + (style_loss * Constants.STYLE_WEIGHT)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-6MBUJbTeiK"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5175,
     "status": "ok",
     "timestamp": 1621584806002,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "vX9Av196aLzX"
   },
   "outputs": [],
   "source": [
    "def train(model, content, style, generated, epochs = 10):\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-4) \n",
    "  for epoch in range(epochs):\n",
    "    with tf.GradientTape() as GT:\n",
    "      loss = Train.calculate_step_loss(model, content, style, generated)\n",
    "\n",
    "    print(f\"EPOCH: {epoch + 1} \\nLOSS: {loss}\\n\" + (\"---\" * 15))\n",
    "\n",
    "    gradients = GT.gradient(loss, generated)\n",
    "    optimizer.apply_gradients([(gradients, generated)])\n",
    "\n",
    "  return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3VlbSdeTaBQ"
   },
   "source": [
    "### Main Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q9rO4x32he1"
   },
   "source": [
    "##### Classsical Style Transfer Approach with VGG19 Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "executionInfo": {
     "elapsed": 24473,
     "status": "error",
     "timestamp": 1621584825330,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "isUoB0YQJ9PB",
    "outputId": "3c6ea6b8-f5da-4c0c-e254-dcd8587a53e0"
   },
   "outputs": [],
   "source": [
    "styled_img_vgg = train(model = FeatureExtractor.vgg_extractor_model,\n",
    "                       content = CONTENT,\n",
    "                       style = STYLE,\n",
    "                       generated = COMBINED,\n",
    "                       epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24432,
     "status": "aborted",
     "timestamp": 1621584825309,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "Bn5iOqitKYdX"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Styled Image - Classical VGG19 Extractor\")\n",
    "plt.imshow(tf.squeeze(styled_img_vgg, axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFmEyEtN2oIJ"
   },
   "source": [
    "##### Style Transfer with Recurrent Convolution Layers (ConvLSTM2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24429,
     "status": "aborted",
     "timestamp": 1621584825314,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "TviJauAq2tyF"
   },
   "outputs": [],
   "source": [
    "styled_img_rnn = train(model = FeatureExtractor.vgg_with_recurrent,\n",
    "                   content = CONTENT,\n",
    "                   style = STYLE,\n",
    "                   generated = COMBINED,\n",
    "                   epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24425,
     "status": "aborted",
     "timestamp": 1621584825317,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "E3tY3key21bc"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Styled Image - Recurrent Convolutional Layers\")\n",
    "plt.imshow(tf.squeeze(styled_img_rnn, axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6qACx023mk"
   },
   "source": [
    "##### Style Transfer with custom convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24417,
     "status": "aborted",
     "timestamp": 1621584825320,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "mYoosEb_28qQ"
   },
   "outputs": [],
   "source": [
    "styled_img_custom = train(model = FeatureExtractor.custom_model,\n",
    "                          content = CONTENT,\n",
    "                          style = STYLE,\n",
    "                          generated = COMBINED,\n",
    "                          epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24415,
     "status": "aborted",
     "timestamp": 1621584825324,
     "user": {
      "displayName": "Ege SabancÄ±",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjZEe-ELL6j-gK8HuoNMKzOi4LsHKLMO5PXg1m3Ug=s64",
      "userId": "11894808898146650716"
     },
     "user_tz": -180
    },
    "id": "P1L50SdS3HEM"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Styled Image - Custom Feature Extractor\")\n",
    "plt.imshow(tf.squeeze(styled_img_custom, axis = 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNcZdFchia/tenyhiig1HEH",
   "collapsed_sections": [],
   "mount_file_id": "1g76hMyrQp1b6leFttWA_OvwJKJJ80Pp3",
   "name": "neural-style-transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
